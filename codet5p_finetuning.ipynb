{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay74Lc2IL7MX"
      },
      "source": [
        "#### 1. Install all requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ9OIv1oL7Ma"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PElIl1RiL7Md"
      },
      "source": [
        "#### 2. Get data from open-source library TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdAhSnsSL7Me"
      },
      "source": [
        "Firstly, we need to download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EVxOjKdL7Mg",
        "outputId": "1c150b69-1aee-4c48-a2fa-493c609e0726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "method name:  Assert\n",
            "method code:\n",
            " @tf_export(\"debugging.Assert\", \"Assert\")\n",
            "@dispatch.add_dispatch_support\n",
            "@tf_should_use.should_use_result\n",
            "def Assert(condition, data, summarize=None, name=None):\n",
            "  \"\"\"Asserts that the given condition is true.\n",
            "\n",
            "  If `condition` evaluates to false, print the list of tensors in `data`.\n",
            "  `summarize` determines how many entries of the tensors to print.\n",
            "\n",
            "  Args:\n",
            "    condition: The condition to evaluate.\n",
            "    data: The tensors to print out when condition is false.\n",
            "    summarize: Print this many entries of each tensor.\n",
            "    name: A name for this operation (optional).\n",
            "\n",
            "  Returns:\n",
            "    assert_op: An `Operation` that, when executed, raises a\n",
            "    `tf.errors.InvalidArgumentError` if `condition` is not true.\n",
            "    @compatibility(eager)\n",
            "    returns None\n",
            "    @end_compatibility\n",
            "\n",
            "  Raises:\n",
            "    @compatibility(TF1)\n",
            "    When in TF V1 mode (that is, outside `tf.function`) Assert needs a control\n",
            "    dependency on the output to ensure the assertion executes:\n",
            "\n",
            "  ```python\n",
            "  # Ensure maximum element of x is smaller or equal to 1\n",
            "  assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])\n",
            "  with tf.control_dependencies([assert_op]):\n",
            "    ... code using x ...\n",
            "  ```\n",
            "\n",
            "    @end_compatibility\n",
            "  \"\"\"\n",
            "  if context.executing_eagerly():\n",
            "    if not condition:\n",
            "      xs = ops.convert_n_to_tensor(data)\n",
            "      data_str = [_summarize_eager(x, summarize) for x in xs]\n",
            "      raise errors.InvalidArgumentError(\n",
            "          node_def=None,\n",
            "          op=None,\n",
            "          message=\"Expected '%s' to be true. Summarized data: %s\" %\n",
            "          (condition, \"\\n\".join(data_str)))\n",
            "    return\n",
            "\n",
            "  with ops.name_scope(name, \"Assert\", [condition, data]) as name:\n",
            "    xs = ops.convert_n_to_tensor(data)\n",
            "    if all(x.dtype in {dtypes.string, dtypes.int32} for x in xs):\n",
            "      # As a simple heuristic, we assume that string and int32 are\n",
            "      # on host to avoid the need to use cond. If it is not case,\n",
            "      # we will pay the price copying the tensor to host memory.\n",
            "      return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")  # pylint: disable=protected-access\n",
            "    else:\n",
            "      condition = ops.convert_to_tensor(condition, name=\"Condition\")\n",
            "\n",
            "      def true_assert():\n",
            "        return gen_logging_ops._assert(  # pylint: disable=protected-access\n",
            "            condition, data, summarize, name=\"Assert\")\n",
            "\n",
            "      guarded_assert = cond.cond(\n",
            "          condition,\n",
            "          gen_control_flow_ops.no_op,\n",
            "          true_assert,\n",
            "          name=\"AssertGuard\")\n",
            "      if context.executing_eagerly():\n",
            "        return\n",
            "      return guarded_assert.op\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "import tensorflow\n",
        "\n",
        "functions = inspect.getmembers(tensorflow, inspect.isfunction)\n",
        "method_names = [f[0] for f in functions]\n",
        "code_snippets = [inspect.getsource(f[1]) for f in functions]\n",
        "\n",
        "print('method name:', method_names[0])\n",
        "print('method code:\\n', code_snippets[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgHJfqtiL7Mi"
      },
      "source": [
        "Split the data into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-Y5pPxoCL7Mj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_code, eval_code, train_methods, eval_methods = train_test_split(\n",
        "    code_snippets, method_names, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEthMf3NL7Mk"
      },
      "source": [
        "Class to represent the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xK9ZQu9KL7Ml"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Dict, List, Optional\n",
        "import torch\n",
        "\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, code_snippets: List[str], method_names: List[str], tokenizer: AutoTokenizer, max_length: int):\n",
        "        self.inputs = code_snippets\n",
        "        self.targets = method_names\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        input = self.inputs[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        input_ids = self.tokenizer(\n",
        "            input,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.squeeze()\n",
        "\n",
        "        target_ids = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.squeeze()\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": target_ids,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-liFFRML7Mo"
      },
      "source": [
        "#### 3. Set up and fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eV36mRdSL7Mp"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "def eval_model(model: nn.Module, eval_loader: DataLoader, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for batch in eval_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=labels)\n",
        "            loss = outputs[0]\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(eval_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5W73MOBL7Mr",
        "outputId": "543d5359-1a33-473e-8903-95b3ae89ffea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss before fine-tuning: 13.898907279968261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 5.395035743713379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 2, loss: 4.520201873779297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 3, loss: 3.935852289199829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:10<00:00,  2.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 4, loss: 3.281422805786133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 5, loss: 2.5783191680908204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 6, loss: 1.8931793928146363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 7, loss: 1.2813795566558839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 8, loss: 0.819016432762146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, loss: 0.516479742527008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10, loss: 0.342057341337204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 11, loss: 0.243259996175766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 12, loss: 0.18193749189376832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 13, loss: 0.14347895979881287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 14, loss: 0.11736719012260437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 15, loss: 0.09817970842123032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 16, loss: 0.08371521383523942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 17, loss: 0.07290228754281998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 18, loss: 0.06206633150577545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 19, loss: 0.06047484278678894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 20, loss: 0.05937379896640778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 21, loss: 0.05085800513625145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 22, loss: 0.04579186961054802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 23, loss: 0.03767436221241951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 24, loss: 0.0333365224301815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 25, loss: 0.031221393123269083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 26, loss: 0.03253896273672581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 27, loss: 0.026241499185562133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 28, loss: 0.02415339685976505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 29, loss: 0.022435473650693892\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "checkpoint = \"Salesforce/codet5p-220m\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
        "\n",
        "train_dataset = CodeDataset(train_code, train_methods, tokenizer)\n",
        "eval_dataset = CodeDataset(eval_code, eval_methods, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Let's see how our model performs before training\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=True)\n",
        "eval_loss = eval_model(model, eval_loader, device)\n",
        "print(f'loss before fine-tuning: {eval_loss}')\n",
        "\n",
        "for epoch in range(1, 30):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_loader):\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    eval_loss = eval_model(model, eval_loader, device)\n",
        "    print(f\"epoch: {epoch}, loss: {eval_loss}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0HrD9WwL7Mu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
